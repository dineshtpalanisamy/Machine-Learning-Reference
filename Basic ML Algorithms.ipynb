{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b289e1b",
   "metadata": {},
   "source": [
    "# 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d496f20",
   "metadata": {},
   "source": [
    "* Supervised Regression Algorithm\n",
    "\n",
    "* Parametric Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9e416a",
   "metadata": {},
   "source": [
    "# Mapping Function\n",
    "\n",
    "Uses Linear Equation\n",
    "\n",
    "y = bias + coeff.X\n",
    "\n",
    "where y is the target value or response and X is the predictor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be677ccd",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "To find a linear relationship between features and the response.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701beb48",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "Uses Mean Squared Error as a loss function.\n",
    "\n",
    "mse = summation ( (y_pred - y_target)^2 ) / number of observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29abd4d5",
   "metadata": {},
   "source": [
    "# Metric\n",
    "\n",
    "* Uses R2 and Residual Standard Error (RSE) is used as metric\n",
    "\n",
    "* RSE tells how much the predictions deviate from the true regression line. We expect it to be lower.\n",
    "\n",
    "* RSE = underroot ( RSS / n-2 ) where n = number of observations\n",
    "R2 tells how much variability can be explained by the model.\n",
    "\n",
    "R2 = TSS - RSS / TSS \n",
    "\n",
    "TSS = Total Sum of squares = summation ( y_true - y_true_mean)^2\n",
    "\n",
    "RSS ( Residual Sum of Squares ) = Summation (y_pred - y_true)^2\n",
    "\n",
    "TSS is the total amount of variance in the response or target (y_true) before the regression is performed\n",
    "\n",
    "RSS is the amount of variance in the response (y_true) that cannot be explained by the model after regression\n",
    "\n",
    "Hence R2 explains the amount of variance that is explained by the model after the regression.\n",
    "\n",
    "* We expect R2 score to be close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05018cd",
   "metadata": {},
   "source": [
    "# Shortcomings\n",
    "\n",
    "* Highly biased algorithm due to its assumption of linear relationship between response and predictor variable.\n",
    "\n",
    "* Low variance algorithm for the same reason as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede5431",
   "metadata": {},
   "source": [
    "# Improvement\n",
    "\n",
    "* Introducing polynomial features in the form of quadratic or cubic forms. It provides some flexibility and due to which the model is able to explain most of the variance in the data.\n",
    "\n",
    "* L1 and L2 regularization - L1 (Lasso) and L2 (Ridge) helps to reduce the variance in the model in order to reduce complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca3c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a0ddb43",
   "metadata": {},
   "source": [
    "## 2.Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7192a",
   "metadata": {},
   "source": [
    "* Supervised Binary Classification Algorithm\n",
    "\n",
    "* Parametric Algorithm\n",
    "\n",
    "* Best for binary classification (0 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a634d05",
   "metadata": {},
   "source": [
    "# Mapping Function\n",
    "\n",
    "* Uses Sigmoid Function\n",
    "\n",
    "Hypothesis = ( 1 / 1 + e^(-z)) where z = bias + coeff_1.(feature_1) + ... + coeff_n.(feature_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d0f63c",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a74fa8a",
   "metadata": {},
   "source": [
    "* To constrain the output values between 0 and 1 to interpret it as probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d775b0",
   "metadata": {},
   "source": [
    "p ( class = 1 / X ) = 1 / 1 + e^(-z)  where X is the input matrix\n",
    "\n",
    "p ( class = 0 / X ) =  1 - p( class = 1 / X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b88e9b",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dbaf7e",
   "metadata": {},
   "source": [
    "* Uses Log Loss Function . It is also known as Cross Entropy Function.\n",
    "\n",
    "log loss or cross entropy = -y log (Hypothesis) + (1-y) log (1 - Hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8096895",
   "metadata": {},
   "source": [
    "* Coefficients are estimated using Maximum Likelihood Estimation i.e. by minimizing cross entropy or log loss between true distribution and predicted distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03effb6",
   "metadata": {},
   "source": [
    "* Maximum Likelihood Estimation is a method where we try to find the best parameters (coefficients or weights) that can fit our observations. It is basically a process of finding the best distribution that defines the responses (y) for our observations (x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c8f675d",
   "metadata": {},
   "source": [
    "Likelihood = P ( X / parameter ) i.e. \n",
    "\n",
    "given a particular parameter and using that if we draw a distribution with that parameter, is it able to cover our\n",
    "responses for our observations (X).\n",
    "\n",
    "MLE = argmax likelihood "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5a5ddd",
   "metadata": {},
   "source": [
    "* Confusion Matrix - It consists of 4 values\n",
    "\n",
    "True Positive ( TP ) = Number of values which were 1 and model predicted them 1\n",
    "\n",
    "True Negative ( TN ) = Number of values which were 0 and model predicted them 0\n",
    "\n",
    "False Positive ( FP ) = Number of values which were 0 and model predicted them 1\n",
    "\n",
    "False Negative ( FN ) = Number of values which were 1 and model predicted them 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d582e031",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9996537f",
   "metadata": {},
   "source": [
    "* Accuracy - gives the total fraction of correct predictions done by the mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4875dbf3",
   "metadata": {},
   "source": [
    "Accuracy = (TP + TN ) / ( TP + TN + FP + FN )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb72a52",
   "metadata": {},
   "source": [
    "* Precision - tells what fraction of positive predictions was actually correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59881ad",
   "metadata": {},
   "source": [
    "Precision = TP  / ( TP + FP )\n",
    "\n",
    "It is sensitive towards false positives (FP). \n",
    "\n",
    "If the precision of the model is 0.5 that means when it predicts the value 1 it is 50 % correct of all the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283233c0",
   "metadata": {},
   "source": [
    "* Recall - tells us what fraction of actual positives are identified correctly. It is also known as Sensitivity and True Positive Rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9391e5af",
   "metadata": {},
   "source": [
    "Recall = TP / ( TP + FN )\n",
    "\n",
    "It is sensitive towards False Negatives (FN).\n",
    "\n",
    "If the recall is 0.5 that means the model predicts 50 % of positive classes correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72895d2f",
   "metadata": {},
   "source": [
    "# * F1 score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b70708",
   "metadata": {},
   "source": [
    "harmonic mean of precision and recall. The highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, if either the precision or the recall is zero\n",
    "\n",
    "F1 score = 2 * [( Precision * Recall ) / ( Precision + Recall )]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69cccad",
   "metadata": {},
   "source": [
    "# * ROC AUC \n",
    "- plot between True Positive Rate (Recall) and False Positive Rate. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4951c20e",
   "metadata": {},
   "source": [
    "False Positive Rate = FP / ( TN + FP )\n",
    "\n",
    "When AUC = 1, then the classifier is able to perfectly distinguish between all the Positive and the Negative class points correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203605ef",
   "metadata": {},
   "source": [
    "When AUC = 0, then the classifier would be predicting all Negatives as Positives, and all Positives as Negatives.\n",
    "\n",
    "  When 0.5<AUC<1, there is a high chance that the classifier will be able to distinguish the positive class values from the negative class values. This is so because the classifier is able to detect more numbers of True positives and True negatives than False negatives and False positives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a77250c",
   "metadata": {},
   "source": [
    " When AUC=0.5, then the classifier is not able to distinguish between Positive and Negative class points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ff5675",
   "metadata": {},
   "source": [
    "# Shortcomings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d0dcd",
   "metadata": {},
   "source": [
    "* When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7014fc0",
   "metadata": {},
   "source": [
    "* Generally not used for multiple classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8f302a",
   "metadata": {},
   "source": [
    "# Improvement "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19d3265",
   "metadata": {},
   "source": [
    "* Multinomial Logistic Regression which uses Softmax function instead of sigmoid function for the prediction.\n",
    "\n",
    "* SVM (Support Vector Machines) and Linear Discriminant Analysis performs well than logistic regression when the classes are well separated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809cdbc",
   "metadata": {},
   "source": [
    "# 3. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f684c523",
   "metadata": {},
   "source": [
    "* Supervised Multiclass Classification Algorithm.\n",
    "\n",
    "* Parametric Algorithm\n",
    "\n",
    "* Naive Bayes is better suited for categorical input variables than numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1941464",
   "metadata": {},
   "source": [
    "# Mapping Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a01f9e8",
   "metadata": {},
   "source": [
    "* Uses Bayes Theorem i.e. posterior = (prior prob. of class * likelihood )/ marginal probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec64495",
   "metadata": {},
   "source": [
    "p(class/ data) = p(class) * p(data/ class) / p(data)\n",
    "\n",
    "p(class) = prior probability of class i.e. the probability of random observation coming from a certain class. \n",
    "           This is our prior belief about a certain class. Statistically, prior tells us the distribution of \n",
    "           that particular class. We find out this by seeing the distribution of the observation belong to a class \n",
    "           it can be gaussian, binomial etc.\n",
    "\n",
    "           prior is also known as DENSITY FUNCTION\n",
    "\n",
    "p( data/ class) = likelihood i.e. given a class, what is the probability that data belongs to a particular class.\n",
    "\n",
    "p ( class / data) = posterior probability i.e. given a observation how likely we are to see a particular class\n",
    "\n",
    "p( data) = marginal probability or evidence or prior probability of the predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfe3bd1",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135e2caa",
   "metadata": {},
   "source": [
    "* We compare an observation's posterior probability for each possible class. The class with the greatest posterior probability is the predicted one.\n",
    "\n",
    "\n",
    "* As the marginal probability is constant across all the observations, we usually compare the numerator part of the posterior probability.\n",
    "\n",
    "\n",
    "* Hence our objective is to maximise posteriori probability (prior * likelihood) for any observation in any class.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0e5a6b1",
   "metadata": {},
   "source": [
    " Maximise ( prior * Likelihood )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bd832e",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17151c03",
   "metadata": {},
   "source": [
    "* Cross Entropy or negative log-likelihood\n",
    "\n",
    "* Maximum-a-Posteriori probability is used for finding parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6494dbce",
   "metadata": {},
   "source": [
    "# Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5ad86",
   "metadata": {},
   "source": [
    " * For each feature, we need to assume the statistical distribution of the likelihood function p(x/y). Distribution is chosen on the basis of nature of features (continuous, discrete, binary etc.). Common distributions are Gaussian, Multinomial and Bernoulli distributions.\n",
    "\n",
    "\n",
    "\n",
    "* Each feature and its resulting likelihood is independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb05d6",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a63b17c",
   "metadata": {},
   "source": [
    "* Accuracy, Precision, Recall, F1 score etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157bdc43",
   "metadata": {},
   "source": [
    "# Shortcomings\n",
    "\n",
    "* It assumes that one feature (observation) is independent of other features\n",
    "\n",
    "\n",
    "* It gives all the features (observations) same level of importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8684c47",
   "metadata": {},
   "source": [
    "# What is \"Naive\" in Naive Bayes ?\n",
    "\n",
    "It makes a Naive assumption that the occurrence of any observation in any class is independent to the other observation. That means the likelihood (probability of any set of data or observations in any class) is equivalent to the joint probability of the individual observations in that class.\n",
    "\n",
    "Suppose we have observations X = (x1, x2, x3 .... xN) and a particular class, then the likelihood \n",
    "\n",
    "    p ( X = (x1, x2 ...xn) / class ) = p( x1/ class) * p (x2/ class) * ... * p (xn / class)\n",
    "\n",
    "                                     = product i 1 to N  P( x_i / class )\n",
    "\n",
    "\n",
    "We does not take the effect of one feature on another feature into consideration and thats why we call it \"Naive\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b9dd9",
   "metadata": {},
   "source": [
    "# 4. Linear Discriminant Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bc46a6",
   "metadata": {},
   "source": [
    "* Supervised Binary and Multiclass Classification Algorithm\n",
    "\n",
    "\n",
    "* Parametric Algorithm\n",
    "\n",
    "\n",
    "* Used for dimensionality reduction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96206f4d",
   "metadata": {},
   "source": [
    "# Mapping Function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f14156",
   "metadata": {},
   "source": [
    "* Uses Bayes Theorem i.e. posterior = (prior prob. of class * likelihood )/ prior prob. of predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee806e4e",
   "metadata": {},
   "source": [
    "p(class/ data) = p(class) * p(data/ class) / p(data)\n",
    "\n",
    "p(class) = prior probability of class i.e. the probability of random observation coming from a certain class. \n",
    "           This is our prior belief about a certain class. Statistically, prior tells us the distribution of \n",
    "           that particular class. We find out this by seeing the distribution of the observation belong to a class \n",
    "           it can be gaussian, binomial etc.\n",
    "\n",
    "           prior is also known as DENSITY FUNCTION\n",
    "\n",
    "p( data/ class) = likelihood i.e. given a class, what is the probability that data belongs to a particular class.\n",
    "\n",
    "p ( class / data) = posterior probability i.e. given a observation how likely we are to see a particular class\n",
    "\n",
    "p( data) = prior probability of data or observations or predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b3f76",
   "metadata": {},
   "source": [
    "* Now p(data) can also be written in the form of prior * likelihood"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2dbf0095",
   "metadata": {},
   "source": [
    "p(data) = p( data / class) * p(class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e96b068",
   "metadata": {},
   "source": [
    "* Hence substituting p(data) in the above bayes theorem the LDA becomes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ee4c83e",
   "metadata": {},
   "source": [
    "p ( class_k / data) =   p(data / class_k) * p(class_k)  / summation i from 1 to K  p(data/ class_i) * p(class_i)\n",
    "\n",
    "where i = class from 1 to K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a89ec5",
   "metadata": {},
   "source": [
    "# FOR DIMENSIONALITY REDUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe365c8",
   "metadata": {},
   "source": [
    "* It creates new features in a lower dimensional space by calculating the eigenvectors of the product of the within-class scatter matrix and between-class scatter matrix.\n",
    "\n",
    "\n",
    "* Mean vector any class is the set of mean of the observations per feature."
   ]
  },
  {
   "cell_type": "raw",
   "id": "120a2959",
   "metadata": {},
   "source": [
    "For any class or label c,\n",
    "\n",
    "M_c = [mean_feature_1, mean_feature_2, ... mean_feature_3 ]\n",
    "\n",
    "Suppose we have 3 classes and 5 features, then we will have 3 mean vectors of 5 dimensions each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e04d6b",
   "metadata": {},
   "source": [
    "* For any class c, per-class scatter matrix would be the difference between the observations falling in that class and the mean vector of that class."
   ]
  },
  {
   "cell_type": "raw",
   "id": "067069cc",
   "metadata": {},
   "source": [
    "For any class c, \n",
    "\n",
    "S_c = Summation ( x_i - mean_j) \n",
    "\n",
    "for i in range(samples in that class c) \n",
    "\n",
    "for j in range(features in that class c)\n",
    "\n",
    "i.e., S_c = X * M_c, where M_c is the mean vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349ff062",
   "metadata": {},
   "source": [
    "* within-class scatter matrix is the summation of all the per-class scatter matrix."
   ]
  },
  {
   "cell_type": "raw",
   "id": "18adc14a",
   "metadata": {},
   "source": [
    "S_W = Summation (S_c) \n",
    "\n",
    "for c in range (labels or classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40cd74",
   "metadata": {},
   "source": [
    "* Overall Mean is the sum of all the samples divided by the number of samples.\n",
    "\n",
    "\n",
    "\n",
    "* Let N_c be the number of samples in a class c, M_c is the mean vector of the class c, then between-class scatter matrix is"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f15204e5",
   "metadata": {},
   "source": [
    "S_b = Summation ( N_c * (M_c - overall mean) ) \n",
    "\n",
    "for c in range(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd0dee",
   "metadata": {},
   "source": [
    "* Hence between-class scatter matrix is the summation of the product of the number of samples in any class and the difference between mean vector of that class and overall mean.\n",
    "\n",
    "\n",
    "* Matrix of eigenvectors are calculated over the product of S_W and S_B."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4420e89",
   "metadata": {},
   "source": [
    "W = Eigen( S_W * S_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b731258",
   "metadata": {},
   "source": [
    "* These eigenvectors are known as Discriminants. These discriminants explains the major portion of separateness. Corresponding eigenvalues are the measure of separateness. More the value, more is the power of the discriminant (eigenvector) to separate classes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e52e27d",
   "metadata": {},
   "source": [
    "W = [ discriminant 1, discriminant 2, discriminant 3,....]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b834ed86",
   "metadata": {},
   "source": [
    "* These eigenvectors are sorted in the decreasing order of their eigenvalues and we select K top discriminants (eigenvectors) and multiply them with our observations to create new features of projections"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2efefc3",
   "metadata": {},
   "source": [
    "New Features  = [ Z_1, Z_2, Z_3 .... Z_k ] \n",
    "\n",
    "Z_k = X * W\n",
    "\n",
    "where X is the observation vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0defd2b8",
   "metadata": {},
   "source": [
    "** In short LDA is a two stage process, first it does dimensionality reduction to find out the discriminants and then using these discriminants to perform classification (bayes approach) on the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8f88f",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "\n",
    "Maximize the ratio of between-class variance to the within-class variance in a set of data and thereby giving maximal separation between the classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091adb6c",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "* Cross entropy or negative log likelihood.\n",
    "\n",
    "* Maximum-a-Posteriori probability is used for finding parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4429852",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    "\n",
    "* In LDA, we assume that the prior or the density function of data belongs to gaussian (normal) distribution.\n",
    "\n",
    "* We also assume that the variance of data in each of the class is same i.e. the covariance matrix (matrix containing correlation between the features in a specific class) is same for all the class.\n",
    "\n",
    "* But mean is class specific i.e. for each class we have specific mean vector"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8cdb11a4",
   "metadata": {},
   "source": [
    "LDA assumes that the observations within each\n",
    "*  class are drawn from a multivariate Gaussian distribution with a class-\n",
    " *  specific mean vector and a covariance matrix that is common to all K\n",
    "  classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47274033",
   "metadata": {},
   "source": [
    "* This common covariance is also known as \"Pooled Variance\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5775d532",
   "metadata": {},
   "source": [
    "# Metric\n",
    "\n",
    "Accuracy, Precision, Recall, F1 score etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611f5c3",
   "metadata": {},
   "source": [
    "# Shortcomings\n",
    "\n",
    "* It assumes that the covariance matrix is common between all the classes. If this assumption goes wrong, model suffers from high bias.\n",
    "\n",
    "\n",
    "* LDA is better when the number of observations is less so it reduces variance by assuming linear relationship between the predcitor X and response Y. Thus it gives low variance. But when the observations are large in number, we need to increase variance and using LDA which provides low variance can be a bad choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb79d40",
   "metadata": {},
   "source": [
    "# Improvement for LDA\n",
    "\n",
    "* Quadratic Discriminant Analysis (QDA), which like LDA assumes the data distribution as gaussian with a class specific mean vector but unlike LDA it assumes different covariance matrix for each class.\n",
    "\n",
    "\n",
    "* Hence it provides more flexibility that is high variance than LDA and can be used for large number of observations\n",
    "\n",
    "\n",
    "* It has the same formula as LDA just with a different covariance matrix for each class, due to which predictor variable, x, becomes quadratic in nature with respect to the response variable y.\n",
    "\n",
    "\n",
    "\n",
    "* Regularized Discriminant Analysis , it introduces the regularization (alpha) term while estimating the variance (or convariance in case of multiple inputs), moderating the influence of different variables on LDA. Since RDA is a regularization technique, it is particularly useful when there are many features that are potentially correlated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b045a8c",
   "metadata": {},
   "source": [
    "# Why it is known as \"Linear\" ?\n",
    "\n",
    "* As we assumed that the observations follow a normal distribution with class specific mean vector and similar covariance matrix in all the classes, we can take the gaussian formula and calculate the likelihood\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "373f6784",
   "metadata": {},
   "source": [
    "\n",
    "p( data / class ) = Gaussian algorithm with mean of that class and common covariance matrix\n",
    "\n",
    "With a bit of mathematical manipulation, it can be shown that LDA algorithm that is\n",
    "\n",
    "  p ( class_k / data) =   p(data / class_k) * p(class_k)  / summation i from 1 to K  p(data/ class_i) * p(class_i)\n",
    "\n",
    "  where i = class from 1 to K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e96750",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728e1599",
   "metadata": {},
   "source": [
    "* Supervised Classification Algorithm\n",
    "\n",
    "* Can be used for regression also\n",
    "\n",
    "* Parametric Algorithm\n",
    "\n",
    "* Effective for the classification of higher dimensional data.\n",
    "\n",
    "* Mostly used for binary classification.\n",
    "\n",
    "* Defines a hyperplane that distinguishes the observations into different classes by maximum margin. If the data is n dimensional then the hyperplane is n-1 dimensional.\n",
    "\n",
    "* Support vectors are the observations that defines the hyperplane and are present at the margin which is the minimum distance between the observation and the hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eda4b3",
   "metadata": {},
   "source": [
    "# Mapping Function\n",
    "\n",
    "\n",
    "A support vector classifier (SVC) can be represented as below"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3942af61",
   "metadata": {},
   "source": [
    "y i (β 0 + β 1 x i1 + β 2 x i2 + . . . + β p x ip ) ≥ M (1 − ei ), for i from 1 to n (number of observations)\n",
    "\n",
    " where y = {+1, -1}\n",
    "\n",
    " p = number of features\n",
    " e_i = slack variables\n",
    " M = Margin \n",
    "\n",
    " Summation ei <= C \n",
    "\n",
    " C is a hyperparameter whose lower value tends to less margin violations and hence narrow margin and higher value\n",
    " tends to more violations and wide margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fbc7e1",
   "metadata": {},
   "source": [
    "* Slack variables, e_i allow individual observations to be on the wrong side of the margin or the hyperplane. Adding all the slack variable should be less than or equal to C."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6e569f5",
   "metadata": {},
   "source": [
    "if e_i = 0, then ith observation is on the correct side of the margin\n",
    "\n",
    "  if e_i > 0, then ith observation is on the wrong side of the margin\n",
    "\n",
    "  if e_i > 1, then ith observations is on the wrong side of the hyperplane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cbaa0",
   "metadata": {},
   "source": [
    "* C is a tuning parameter. It bounds the sum of slack variables and so it determines the number and severity of the violations to the margin (and to the hyperplane) that we will tolerate.\n",
    "  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb306bd5",
   "metadata": {},
   "source": [
    "If C = 0, no violations and slack variables = 0 \n",
    "\n",
    "  If C > 0 , no more than C observations can be on the wrong side of the hyperplane\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117b318d",
   "metadata": {},
   "source": [
    "* As the budget C increases, we become more tolerant of violations to the margin, and so the margin will widen. Conversely, as C decreases, we become less tolerant of violations to the margin and so the margin narrows.\n",
    "\n",
    "\n",
    "* The above formula is computed using the inner product of the observations as opposed to the observations themselves."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef08e277",
   "metadata": {},
   "source": [
    "inner product of r-vectored  a and b = Summation [ a_i * b_i ] , for i in range(1, r)\n",
    "\n",
    "It is represented as <a, b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d222cc6",
   "metadata": {},
   "source": [
    "* Support Vector Machines are extension to Support vector classifiers that results from enlarging the feature space in a specific way, using kernels. It can be represented as"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2fb22727",
   "metadata": {},
   "source": [
    " f(x) = bias + Summation[ alpha_i * K(x, x_i)] , for i in range(0, p) , p = number of features\n",
    "\n",
    " alpha_i = hyperparameter which is non-zero for support vectors and zero for rest of the observations\n",
    "\n",
    " K(x, x_i) = Kernel Function\n",
    "\n",
    " for linear kernel \n",
    "\n",
    " K(x, x_i) = < x, x_i >\n",
    "\n",
    " for polynomial kernel\n",
    "\n",
    " K(x, x_i) = ( 1 + < x, x_i > ) ^ d , where d = degree of the polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ebde80",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "The main objective of SVM is to find the optimal hyperplane which linearly separates the data points in two component by maximizing the margin ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5248007",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "\n",
    "* Hinge Loss + Penality\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b501724",
   "metadata": {},
   "source": [
    "max(0,1 − y_true* f(x) ) + λ * summation [ β_i^2 ] , for i in range(1, p) , p= number of features\n",
    "\n",
    "It can be understood as Loss + Penality \n",
    "\n",
    "First term is Hinge Loss and Second term is ridge penality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa33b41",
   "metadata": {},
   "source": [
    "* Hinge loss simply tells that for all the observations which has f(x) >=1, loss is zero that is they are on the correct side of the margin. Thus these observations do not play any role in affecting the margin.\n",
    "\n",
    "* For the observations which has f(x) <1, loss increases.\n",
    "\n",
    "* Here, λ is a nonnegative tuning parameter. When λ is large then β 1 , . . . , β p are small, more violations to the margin are tolerated, and a low-variance but high-bias classifier will result. When λ is small then few violations to the margin will occur; this amounts to a high-variance but low-bias classifier.\n",
    "\n",
    "* Small λ means small C.\n",
    "\n",
    "* When C > 0 , then the classifier is called Soft Margin Classifier and when C = 0, then the clasifier is called Hard Margin Classifier.\n",
    "\n",
    "* Slack variables are zero for Hard Margin Classifiers , hence no violations are allowed and all the observations must be on the correct side of the margin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bbe464",
   "metadata": {},
   "source": [
    "# Metric\n",
    "\n",
    "F1 score, accuracy, precision, recall etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8697f",
   "metadata": {},
   "source": [
    "# Shortcomings\n",
    "\n",
    "Not suitable for large datasets\n",
    "\n",
    "Does not perform well on overlapping classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9b85a8",
   "metadata": {},
   "source": [
    "# Difference between Maximum Margin Classifier , Support Vector Classifier and Support Vector Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a27f2b",
   "metadata": {},
   "source": [
    "# Maximum Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1cca5c",
   "metadata": {},
   "source": [
    "* Maximum Margin Classifier is very strict with its margin and does not any observations to fall on the wrong side of margin or the hyperplane. The loss function doesn't include the penality term. There are no slack variables or you can say C parameter is 0. These are also known as Hard Margin Classifiers. It has relatively high bias and low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa966f7",
   "metadata": {},
   "source": [
    "# Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e38c75f",
   "metadata": {},
   "source": [
    "* Support Vector Classifier allows some of the observations to fall on the wrong side of the margin and hyperplane. It provides some flexibility in its margin. The loss functions includes the penality term. C parameter is greater than 0. These are also known as Soft Margin Classifiers. It has relatively low bias and high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e400426",
   "metadata": {},
   "source": [
    "# Support Vector Machines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b51f33",
   "metadata": {},
   "source": [
    "* Support Vector Machines is an extension of support vector classifiers and they introduce kernel functions to accomodate non-linear boundaries. Above two classifiers fails in the case of non-linear boundaries between classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5b54b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae9a16a3",
   "metadata": {},
   "source": [
    "# K Nearest Neighbor ( KNN )\n",
    "\n",
    "* Supervised Classification and Regression Algorithm.\n",
    "\n",
    "* Non-parametric Algorithm\n",
    "\n",
    "* Uses distance metric (e.g. euclidean, manhattan, minkowski, Tanimoto, Jaccard, Mahalanobis, cosine distance etc. ) in classifying an observation i.e. if an observation is nearest to K observations of a certain class, then it allots that observation to be of that class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c6c8c",
   "metadata": {},
   "source": [
    "# Mapping Function\n",
    "\n",
    "* It has no model hence no mathetimatical formulation of KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8670fd9",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "* To find a nearest class for all the observations using distance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9078a68d",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "* No loss function as it is not trained. It just memorises the data and at the time of prediction it performs search and majority vote."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41bfd31",
   "metadata": {},
   "source": [
    "# Metric\n",
    "\n",
    "* Accuracy, F1 score, precision, recall etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784192d6",
   "metadata": {},
   "source": [
    "# Shortcomings\n",
    "\n",
    "* Its good for observations with low dimensions or low features due to curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d47c3",
   "metadata": {},
   "source": [
    "# Curse of Dimensionality "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770bf7bd",
   "metadata": {},
   "source": [
    "* As we know KNN makes decision by calculating the distance between the inputs, when the input variables increase ( features increase), it increases the dimension and calculating distance between two points in multi-dimensional fields becomes difficult. Points which might look similar may be very far away in distance. This phenomenon of unable to identify similar points in multi-dimensional space is known as Curse of Dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1816d7d",
   "metadata": {},
   "source": [
    "* Missing values need to be removed as they can be hindrance in calculating distance between samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea1ef0",
   "metadata": {},
   "source": [
    "# Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c903b4c3",
   "metadata": {},
   "source": [
    "* Reducing features using feature selection techniques or using PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca29d784",
   "metadata": {},
   "source": [
    "# What is \"K\" in KNN ?\n",
    "K is the minimum number of observations required to classify an observation into a certain class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd85f03",
   "metadata": {},
   "source": [
    "# Why KNN is a lazy learner ?\n",
    "\n",
    "As there is no model i.e. we don't have any mapping function or mathematical model, hence no learning happens. All the process happens at the time of the prediction. Hence its called a lazy learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85df9e69",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "* Supervised Classification and Regression algorithm\n",
    "\n",
    "* Non-parametric algorithm\n",
    "\n",
    "* Most interpretable algorithm.\n",
    "\n",
    "* Uses a tree structure to make decisions. Every node represents a decision through which samples can be divided into different branches and leaves represents the classes in the case classification or a numerical value in case of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b0db4",
   "metadata": {},
   "source": [
    "# Mapping Function\n",
    "\n",
    "No mapping function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7610fb5d",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "\n",
    "* Stratifying or segmenting the predictor space into a number of simple regions (leaves or terminal nodes). Uses recursive binary splitting that is the top down greedy approach to successively split the predictor space into branches until a stopping criterion is meet.\n",
    "\n",
    "* In case of regression, the leaves are the mean value of the observations falling in that region. In case of classification, the leaves are the classes to which the observations belong to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554d8a97",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "For regression, Residual Square of Sum\n",
    "\n",
    "For classification, gini impurity and entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95fb2b",
   "metadata": {},
   "source": [
    "# gini impurity = Summation [ P_mk ( 1- P_mk ) ] from k = i to C classes\n",
    "\n",
    "P_mk = proportion of training observations in the mth region (leaf) that are from the kth class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f65280",
   "metadata": {},
   "source": [
    "\n",
    "# Entropy = - Summation [ P_mk log P_mk ] from k=i to C classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ca052f",
   "metadata": {},
   "source": [
    "* Process of reducing the entropy is known as Information Gain. Lower the entropy, higher the information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de116d",
   "metadata": {},
   "source": [
    "# Shortcomings\n",
    "\n",
    "* Very likely to overfit the data beacuse of the complex tree structure hence high variance.\n",
    "\n",
    "* It is very non robust. A a small change in the data can cause a large change in the final estimated tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e7193d",
   "metadata": {},
   "source": [
    "# Improvement\n",
    "\n",
    "* Cost complexity pruning - This technique helps in reducing overfitting as it prunes the optimal subtree out of the large complex tree. It adds a penality term (similar to the lasso term) in the cost function or the loss function."
   ]
  },
  {
   "cell_type": "raw",
   "id": "98037c3f",
   "metadata": {},
   "source": [
    "Penality term = alpha * |T| \n",
    "\n",
    "where alpha is the tuning parameter and |T| is the number of leaves of the tree\n",
    "\n",
    "as αlpha increases, there is a price to pay for having a tree with many terminal nodes that is the branches\n",
    "get pruned from the tree in a nested and predictable fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307f9ace",
   "metadata": {},
   "source": [
    "# BAGGING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2653bc",
   "metadata": {},
   "source": [
    "* Bagging technique that is Bootstrap Aggregation where we build separate decision trees using bootstrapped set of samples and average the resulting predictions. Each individual decision tree are grown deep without any pruning and hence each of them has high variance and low bias but averaging them reduces the overall variance. They result in improved accuracy over prediction with a single tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c59e16",
   "metadata": {},
   "source": [
    "# Disadvantage of bagging\n",
    "\n",
    "* Bagging technique suffers from a disadvantage that of any of the predictor is very very strong than the other predictors. Each bagged tree will look similar because most of them will use that strong predictor. Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3b432",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75df7c8",
   "metadata": {},
   "source": [
    "* Random Forest overcome this problem by forcing each split to consider only a subset of the predictors that are random. The main difference between bagging and random forests is the choice of predictor subset size. If a random forest is built using all the predictors, then it is equal to bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69134b6c",
   "metadata": {},
   "source": [
    "# * Boosting\n",
    "\n",
    "* Boosting works in a similar way, except that the trees are grown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set. Unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown. Because the growth of a particular tree takes into account the other trees that have already been grown, smaller trees are typically sufficient. These smal trees are mostly Stump which have single split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cc6cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d108b03",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af68a51",
   "metadata": {},
   "source": [
    "* Unsupervised dimensionality reduction technique. Dimensionality reduction comes under feature extraction.\n",
    "\n",
    "* It creates new features using the linear combination of the original features. These new features are called projections. These projections are orthogonal means they are independent of each other or we can say they are linearly uncorrelated. They preserves the most important structure or relationships between the variables observed in the data.\n",
    "\n",
    "* These projections are also known as principal component vectors. Athough there is subtelty as in what is exactly principal component, please refer to this answer. for more details.\n",
    "\n",
    "* Singular Value Decomposition is used to calculate the principal components and they are usually ranked in the order of the amount of variance they explain. First principal component explains the most variance followed by second and then third.\n",
    "\n",
    "* So, PCA is the dimensionality reduction technique that creates linearly uncorrelated (orthogonal) features with a dimension less than the original features. If originally you had 678 features, it might reduce to 200 or 100 features.\n",
    "\n",
    "* We can also say that PCA maps the higher dimensional data into lower dimensional space. Due to this property, PCA is often used in Exploratory Data Analysis (which is a fancy word for visualizing the data). It is often difficult to visualise higher dimensional data and thus using PCA you can reduce the dimensions and can plot the new features.\n",
    "\n",
    "* Before PCA it is necessary to do standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40254b9",
   "metadata": {},
   "source": [
    "# Mapping Function\n",
    "\n",
    "* Singular Value Decomposition (SVD) or Eigen Decomposition is used to perform PCA.\n",
    "\n",
    "* Standardise the data.\n",
    "\n",
    "* Find the covariance matrix of all the features.\n",
    "\n",
    "* Perform SVD to find eigenvectors and eigenvalues.\n",
    "\n",
    "* Sort the eigenvectors in the decreasing eigenvalues. More the eigenvalue, more variance is explained by that eigenvector.\n",
    "\n",
    "* Select top k eigenvectors and project your observations over it. By projecting means take a dot product of the observation and top k eigenvectors to create projections.\n",
    "\n",
    "* These projections are the new features that explain the maximum variance.\n",
    "\n",
    "* Eigenvectors are called principal components and the projections (after dot product of observation & eigenvalue) is called principal component vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfdcbc0",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "* To create new linearly uncorrelated features which explains the most of the variance.\n",
    "\n",
    "* To reduce the dimensions(features) of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840da898",
   "metadata": {},
   "source": [
    "# Advantages\n",
    "\n",
    "* It removes correlated features.\n",
    "\n",
    "* Improves model performance as it contains less features.\n",
    "\n",
    "* Reduces overfitting.\n",
    "\n",
    "* Improves visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d86cb7",
   "metadata": {},
   "source": [
    "# Shortcomings\n",
    "\n",
    "* Principal components (new features) are less interpretable. The original features is explainable but when they are transformed into component vectors, it just appears a bunch of numerical numbers. They don't have any column they are just \"first component vector\", \"second component vector\", and so on.\n",
    "\n",
    "* We know that it reduces the features lets say from N to M where M < N but how do we choose M. If M is not choosen correctly we might miss some information. Hence there is a chance of information loss.\n",
    "\n",
    "* PCA gives the linear combinations of the features. But what if the features are not linearly separable. PCA fails there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc5d16e",
   "metadata": {},
   "source": [
    "# Improvement\n",
    "\n",
    "* KernelPCA which uses the kernel function to map the non-linear observations into a higher dimensional space where they are linear separable and then apply PCA in that higher dimensional to map it back to a lower dimension where it can separate the samples using any linear classifier. It is also computationally expensive.\n",
    "\n",
    "* Commonly used kernels are polynomial kernel, radial basis (gaussian kernel) kernel, Sigmoid (hyperbolic tangent) kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d21e505",
   "metadata": {},
   "source": [
    "# Why PCA is unsupervised technique ?\n",
    "\n",
    "It creates the linear projections using only the features and not the target variable. It doesn't use the target column while creating principal components. Hence it falls under unsupervised category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c341d4ad",
   "metadata": {},
   "source": [
    "# How is LDA (linear discriminant analysis) is different from PCA ?\n",
    "\n",
    "* LDA is a supervised dimensionality reduction whereas PCA is unsupervised.\n",
    "\n",
    "* Both of them uses Singular Value Decomposition to calculate the eigenvectors and eigenvalues.\n",
    "\n",
    "* LDA applies SVD to the scatter matrix whereas PCA applies SVD to covariance matrix.\n",
    "\n",
    "* Eigenvectors in LDA is known as discriminants and is a measure of class separateness, whereas eigenvectors in PCA is known as principal components and is a measure of variance explained by the features.\n",
    "\n",
    "* LDA is further used for classification purposes using Bayes theorem. PCA only does feature reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e2b2",
   "metadata": {},
   "source": [
    "# K Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262bc948",
   "metadata": {},
   "source": [
    "* Unsupervised algorithm to create clusters or groups among the data.\n",
    "\n",
    "* Non-parametric Algorithm.\n",
    "\n",
    "* When data doesn't have any target variable, then we use this algorithm. It finds the pattern among the features and groups them into K clusters on the basis of the similarity. A cluster is a group of similar observations.\n",
    "\n",
    "* Uses distance metric (e.g. euclidean, manhattan, minkowski, Tanimoto, Jaccard, Mahalanobis, cosine distance etc. ) to calculate similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a3cb62",
   "metadata": {},
   "source": [
    "# Mapping Function\n",
    "\n",
    "It has no mathetimatical formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2524ca2",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "Group all the observations into K clusters using distance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e606b59",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "* It tries to minimise the distance between the centroid of a cluster & the observations falling into that cluster and tries to maximise the distance between various clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e2920b",
   "metadata": {},
   "source": [
    "# Metric\n",
    "\n",
    "* Sillhouette coefficient - a measure of how close each point in one cluster is to points in the neighboring clusters. This is in the range of [-1,1].\n",
    "\n",
    "* Values near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2213962",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    "\n",
    "* It assumes that the clusters are convex (circle or sphere)\n",
    "\n",
    "* All features are equally scaled.\n",
    "\n",
    "* The groups are balanced and they have same number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7b909",
   "metadata": {},
   "source": [
    "# Shortcomings\n",
    "\n",
    "* If the above assumptions are not met, then k-means is not effective.\n",
    "\n",
    "* We need to manually choose the value of K (number of clusters). Usually 3 or 5 is taken.\n",
    "\n",
    "* Computationally expensive as it takes all the training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc49a44",
   "metadata": {},
   "source": [
    "# Improvement\n",
    "\n",
    "* Computational cost can be reduces using Mini-batch k means clustering.\n",
    "\n",
    "* Meanshift clustering overcomes the assumption of shape and the K value. We don't need to manually choose K value in meanshift.\n",
    "\n",
    "* DBSCAN (Density Based Spatial Clustering of Applications with Noise) selects clusters on the basis of high density and overcomes the assumption of convex shape.\n",
    "\n",
    "* Agglomerative Hierarchical is a powerful and flexible hierarchical algorithm in which all the observations start as their own clusters and they keep on merging until a criteria is meet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f4b2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c527b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa229c84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
